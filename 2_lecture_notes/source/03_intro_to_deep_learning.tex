\documentclass[11pt]{beamer}
\usetheme{CambridgeUS}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{pgfpages}
\usepackage{framed}
\usepackage{xcolor}
\usepackage[most]{tcolorbox}
\usepackage{soul}
\usepackage{empheq}

% The replacement character ï¿½ (often displayed as a black rhombus with a white
% question mark) is a symbol found in the Unicode standard at code point U
% +FFFD in the Specials table. It is used to indicate problems when a system 
% is unable to render a stream of data to a correct symbol.[4] It is usually 
% seen when the data is invalid and does not match any character. For this 
% reason we map explicitly this character to a blanck space.
\DeclareUnicodeCharacter{FFFD}{ }

\newcommand*{\itemimg}[1]{%
  \raisebox{-.3\baselineskip}{%
    \includegraphics[
      height=\baselineskip,
      width=\baselineskip,
      keepaspectratio,
    ]{#1}%
  }%
}

\newtcbox{\mymath}[1][]{%
    nobeforeafter, math upper, tcbox raise base,
    enhanced, colframe=blue!30!black,
    colback=blue!10, boxrule=1pt,
    #1}

\newcommand{\highlight}[1]{%
  \colorbox{yellow!100}{$\displaystyle#1$}}

\author{Giovanni Della Lunga\\{\footnotesize giovanni.dellalunga@unibo.it}}
\title{3 - Introduction to Deep Learning}
%\title{4 - Basic Text Analysis}
%\title{5 - Introduction to Natural Language Processing}
%\title{7 - Classification for Text Analysis}
%\title{8 - Clustering for Text Similarity}
\subtitle{} % (optional)
\setbeamercovered{transparent} 
\institute{Halloween Conference in Quantitative Finance} 
\date{Bologna - October 26-28, 2021} 

\begin{document}

\begin{frame}
\includegraphics[width=\linewidth]{img/halloween-seminar-logo.PNG}
\end{frame}

\begin{frame}
\titlepage
\end{frame}

\AtBeginSection[]
{
  %\begin{frame}<beamer>
  %\footnotesize	
  %\frametitle{Outline}
  %\begin{multicols}{2}
  %\tableofcontents[currentsection]
  %\end{multicols}	  
  %\normalsize
  %\end{frame}
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}  	\usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\AtBeginSubsection{\frame{\subsectionpage}}

% INSERT HERE

\begin{frame}{We will talk about...}
\begin{itemize}
\item What is a Neural Network
\item Feedforward Neural Networks
\item Keras: the Python Deep Learning API  
\end{itemize}
\end{frame}

\subsection{Machine Learning and Deep Learning \\ \scalebox{0.8}{	}}
%---------------------------------------------------------------------------------------------------
\begin{frame}{Introduction: Machine Learning and Deep Learning}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item As we know to do machine learning we need three things:
		\item Input data points
		\item Examples of the expected output
		\item A way to measure whether the algorithm is doing a good job
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../../07-pictures/03_intro_to_deep_learning_pic_0.png}
    %}
\end{column}%
\end{columns}
\end{frame}
\begin{frame}{Introduction: Machine Learning and Deep Learning}
	\begin{itemize}
		\item A machine-learning model transforms its input data into meaningful outputs, a process that is \textbf{learned} from exposure to known examples of inputs and outputs. 
		\item Therefore, the central problem in machine learning and deep learning is to \textbf{meaningfully transform data}: in other words, \textbf{to learn useful representations of the input data at hand, representations that get us closer to the expected output}. 
		\item What is a representation? At its core, it is simply a different way to look at data, to represent or encode data.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Introduction: Machine Learning and Deep Learning}
	Deep learning is a specific subfield of machine learning: a new take on learning representations from data that puts an emphasis on learning successive layers of increasingly meaningful representations.
	\begin{center}
	\includegraphics[scale=0.7]{../../07-pictures/03_intro_to_deep_learning_pic_1.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Introduction: Machine Learning and Deep Learning}
	Deep learning is a specific subfield of machine learning: a new take on learning representations from data that puts an emphasis on learning successive layers of increasingly meaningful representations.
	\begin{center}
	\includegraphics[scale=0.5]{../../07-pictures/03_intro_to_deep_learning_pic_2.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Introduction: Machine Learning and Deep Learning}
	In deep learning, these layered representations are (almost always) learned via models called \textbf{neural networks}, structured in literal layers stacked on top of each other.
	\begin{center}
	\includegraphics[scale=0.5]{../../07-pictures/03_intro_to_deep_learning_pic_3.png}
	\end{center}
\end{frame}
%..................................................................
\subsection{The McCulloch-Pitts Neuron \\ \scalebox{0.8}{	}}
%---------------------------------------------------------------------------------------------------
\begin{frame}{Mc-Culloch and Pitts Neuron}
	\begin{center}
	\includegraphics[scale=0.5]{../../07-pictures/03_intro_to_deep_learning_pic_4.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_5.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_6.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_7.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Mc-Culloch and Pitts Neuron}
From a functional point of view
	\begin{itemize}
		\item an input signal formally present but associated with a zero weight is equivalent to an absence of signal;
		\item the threshold can be considered as an additional synapse, connected in input with a fixed weight equal to 1;
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_8.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_9.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_10.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{NN Data Flow}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_11.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
\begin{equation} a = \sum\limits_{i=1}^n w_i \, x_i - \theta \end{equation}
\begin{equation} y = f(a) = \begin{cases} 0, \quad \text{if} \, a \le 0 \\ 1, \quad \text{if} \, a > 0\end{cases} \end{equation}
	\begin{itemize}
		\item The function $f$ is called the response or activation function:
		\item in the McCulloch and Pitts neuron $f$ is simply the step function, so the answer is binary: it is $1$ if the weighted sum of the stimuli exceeds the internal threshold; $0$ otherwise.
		\item Other models of artificial neurons predict continuous response functions
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_12.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Activation Function}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_13.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Neural Network Basic Constituents}
	\begin{itemize}
		\item A neural network consists of:
		\item A set of nodes (neurons), or units connected by links.
		\item A set of \textbf{weights} associated with links.
		\item A set of thresholds or activation levels.
		\item Neural network design requires:
		\item 1. The choice of the number and type of units.
		\item 2. The determination of the morphological structure.
		\item 3. Coding of training examples, in terms of network inputs and outputs.
		\item 4. Initialization and training of weights on interconnections, through the set of learning examples.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Neural Network Basic Constituents}
	\begin{itemize}
		\item The specification of what a layer does to its input data is stored in the layer s weights, which in essence are a bunch of numbers. 
		\item In technical terms, we could say that the transformation implemented by a layer is parameterized by its weights (Weights are also sometimes called the parameters of a layer.) 
		\item In this context, \textbf{learning means finding a set of values for the weights of all layers in a network, such that the network will correctly map example inputs to their associated targets}.
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Loss Function}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item To control the output of a neural network, you need to be able to measure how far this output is from what you expected. 
		\item This is the job of the \textbf{loss function} of the network, also called the objective function. 
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../../07-pictures/03_intro_to_deep_learning_pic_14.png}
    %}
\end{column}%
\end{columns}
\end{frame}
\begin{frame}{Loss Function}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item The loss function takes the predictions of the network and the true target (what you wanted the network to output) and \textbf{computes a distance score, capturing how well the network has done on this specific example}
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../../07-pictures/03_intro_to_deep_learning_pic_15.png}
    %}
\end{column}%
\end{columns}
\end{frame}
\begin{frame}{Loss Function}
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_16.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Backpropagation}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item The fundamental trick in deep learning is to use this score as a feedback signal to adjust the value of the weights a little, in a direction that will lower the loss score for the current example. 
		\item This adjustment is the job of the optimizer, which implements what is called the Backpropagation algorithm: the central algorithm in deep learning.
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../../07-pictures/03_intro_to_deep_learning_pic_17.png}
    %}
\end{column}%
\end{columns}
\end{frame}
\begin{frame}{Implementing a Single Layer NN}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
		In each hidden unit, take $a_1$ as example, a linear operation followed by an activation function, $f$, is performed. So given input $x = (x_1, x_2)$, inside node $a_1$, we have:
		$$z_1 = w_{11}x_1 + w_{12}x_2 + b_1$$
		$$a_1 = f(w_{11}x_1 + w_{12}x_2 + b_1) = f(z_1) $$
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../../07-pictures/03_intro_to_deep_learning_pic_18.png}
    %}
\end{column}%
\end{columns}
\end{frame}
\begin{frame}{Implementing a Single Layer NN}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
		Same for node $a_2$, it would have:
		$$z_2 = w_{21}x_1 + w_{22}x_2 + b_2$$
		$$a_2  = f(w_{21}x_1 + w_{22}x_2 + b_2) = f(z_2)$$
		And same for $a_3$ and $a_4$ and so on
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../../07-pictures/03_intro_to_deep_learning_pic_19.png}
    %}
\end{column}%
\end{columns}
\end{frame}
\begin{frame}{Implementing a single Layer NN}
We can also write in a more compact form
\begin{equation}
\begin{pmatrix}
z_1 \\ z_2 \\ z_3 \\ z_4
\end{pmatrix} =
\begin{pmatrix}
w_{11} & w_{12} \\ w_{21} & w_{22} \\ w_{31} & w_{32} \\ w_{41} & w_{42}
\end{pmatrix} 
\cdot 
\begin{pmatrix}
x_1 \\ x_2 
\end{pmatrix}
+
\begin{pmatrix}
b_1 \\ b_2 \\ b_3 \\ b_4
\end{pmatrix} 
\Rightarrow Z^{[1]} = W^{[1]} \cdot X + B^{[1]} 
\end{equation}
\vspace{0.5cm}

\textbf{The output is one value $y_1$ in $[0, 1]$, consider this a binary classification task with a prediction of probability}
\end{frame}
%..................................................................
\begin{frame}{Implementing a single Layer NN}
Let's assume that the first activation function is the $\tanh$ and the output activation function is the $sigmoid$. So the result of the hidden layer is:
$$ A^{[1]} = \tanh{Z^{[1]}} $$
This result is applied to the output node which will perform another linear operation with a different set of weights, $W^{[2]}$:
$$ Z^{[2]} = W^{[2]} \cdot A^{[1]} + B^{[2]} $$
and the final output will be the result of the application of the output node activation function (the sigmoid) to this value:
$$ \hat{y} = \sigma({Z^{[2]}}) = A^{[2]}$$
\end{frame}
%..................................................................
\begin{frame}{Activation Functions}
Function tanh and sigmoid looks as below. Notice that the only difference of these functions is the scale of y.
	\begin{center}
	\includegraphics[scale=0.7]{../../07-pictures/03_intro_to_deep_learning_pic_19_b.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Logistic Loss Function}
Since we have a binary classification problem, we can assume a Logistic Loss Function (see the problem of logistic regression)
\begin{equation}
L(y, \hat{y}) = 
\begin{cases} 
-\log{\hat{y}} & \text{when}\, y = 1 \\ -\log(1 - \hat{y}) & \text{when}\, y = 0 
\end{cases} 
\end{equation}
$$ L(y, \hat{y}) = -[y\log{\hat{y}} + (1 - y)\log{(1 - \hat{y})}] $$

Where $\hat y$ is our \highlight{\text{prediction}} ranging in $[0, 1]$ and $y$ is the \highlight{\text{true}} value. 

\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
\begin{itemize}
\item Given a generic actual value $y$, we want to minimize the loss $L$, and the technic we are going to apply here is gradient descent; 
\item basically what we need to do is to apply derivative to our variables and move them slightly down to the optimum. 
\item Here we have 2 variables, $W$ and $b$, and for this example, the update formula of them would be:

\begin{align*}
&W = W - \frac{\partial L}{\partial W} \\
&b = b - \frac{\partial L}{\partial b}
\end{align*}

\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
\begin{itemize}
\item The delta rule algorithm works by computing the gradient of the loss function with respect to each weight. 
\item Remember that
\begin{align*}
\hat y &= A^{[2]} = \sigma(Z^{[2]} = \sigma \left( W^{[2]} \cdot A^{[1]} + B^{[2]} \right) \\
&= \sigma \left( W^{[2]} \cdot \tanh{Z^{[1]}} + B^{[2]} \right) \\
&= \sigma \left[ W^{[2]} \cdot \tanh\left( W^{[1]} \cdot X + B^{[1]} \right) + B^{[2]} \right]
\end{align*}
\item As you can see $\hat y$ depends on both $W^{[1]}$ and $W^{[2]}$. 
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Delta Rule}
\begin{itemize}
\item In order to get the derivative of our targets, chain rules would be applied:
\begin{align*}
&\frac{\partial L}{\partial W} =  \frac{\partial L}{\partial \hat y} \frac{\partial \hat y}{\partial Z} \frac{\partial Z}{\partial W} \\
&\frac{\partial L}{\partial b} =  \frac{\partial L}{\partial \hat y} \frac{\partial \hat y}{\partial Z} \frac{\partial Z}{\partial b} 
\end{align*}
\item In particular the derivative of the Loss Function with respect to $\hat y$ is very easy and can be calculated once for all because it does not depend on the particular layer:
\begin{align*}
&L(y, \hat{y}) = -[y\log{\hat{y}} + (1 - y)\log{(1 - \hat{y})}] \\
&\frac{\partial L}{\partial \hat y} = -\frac{y}{\hat y} + \frac{1-y}{1-\hat y} = \frac{\hat y - y}{\hat y(1 - \hat y)} \\
&= \frac{A^{[2]} - y}{A^{[2]}(1 - A^{[2]})}
\end{align*}
\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation}
\textbf{Hidden Layer Activation Function (Hyperbolic Tangent)}
\begin{equation}
\tanh x = \frac{{{e^x} -{e^{- x}}}}{{{e^x} + {e^{ -x}}}} 
\Rightarrow 
\frac{d}{{dx}}\tanh x =  1-{\left(\tanh x \right)}^2 
\end{equation}
\textbf{Output Layer Activation Function (Sigmoid Function)} 
\begin{equation}
\sigma(x) =  \left[ \dfrac{1}{1 + e^{-x}} \right]  \Rightarrow
\dfrac{d}{dx} \sigma(x) =  \sigma(x) \cdot (1 - \sigma(x))
\end{equation}
\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation}
\textbf{Output Layer}
\begin{align*}
& \frac{\partial L}{\partial A^{[2]}} = \frac{A^{[2]} - y}{A^{[2]}(1 - A^{[2]})} \notag\\
& \frac{\partial A^{[2]}}{\partial Z^{[2]}} = \frac{\partial \sigma(Z^{[2]})}{\partial Z^{[2]}} = \sigma(Z^{[2]}) \cdot (1 - \sigma(Z^{[2]})) 
= A^{[2]} (1 - A^{[2]}) \\
& \frac{\partial Z^{[2]}}{\partial W^{[2]}} = A^{[1]} 
\end{align*}
So the complete gradient is:

\begin{align*}
\frac{\partial L}{\partial W^{[2]}} &=  
\frac{A^{[2]} - y}{A^{[2]}(1 - A^{[2]})} \cdot 
A^{[2]} (1 - A^{[2]}) \cdot {A^{[1]}}^T \\
&= (A^{[2]} - y) \cdot {A^{[1]}}^T
\end{align*}

\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation}
\textbf{Hidden Layer}
Now we have to calculate
$$\frac{\partial Z^{[2]}}{\partial W^{[1]}}$$
Remember that
$$Z^{[2]} = W^{[2]} \cdot tanh\left( W^{[1]} \cdot X + b^{[1]} \right) + b^{[2]}$$
and
$$\frac{\partial Z^{[2]}}{\partial W^{[1]}} = W^{[2]} \cdot \frac{\partial \, tanh(\dots)}{\partial W^{[1]}} \cdot X = W^{[2]} \cdot \left( 1 - tanh^2(\dots) \right) \cdot X$$
\end{frame}
%..................................................................
\begin{frame}{Gradient Calculation}
\textbf{Hidden Layer}\\
\vspace{0.5cm}
Finally
\begin{align*}
\frac{\partial L}{\partial W^{[1]}} &= (A^{[2]} - y) \cdot W^{[2]} \cdot X \cdot \left( 1 - tanh^2(\dots) \right) \\
&= (A^{[2]} - y) \cdot W^{[2]} \cdot X \cdot \left( 1 - {A^{[1]}}^2 \right)
\end{align*}
\end{frame}
%..................................................................
\begin{frame}{Weights Update}
Defining
\begin{align} 
& \Delta^{[2]} = A^{[2]} - Y  \\
& \Delta^{[1]} = \Delta^{[2]} \cdot W^{[2]T} \cdot (1 - A^{[1]^2})
\end{align}
We have\\
\vspace{0.5cm} 
\textbf{Output Layer}
\begin{align}
& dW^{[2]} = \frac{1}{m}\left[A^{[2]} - Y \right]A^{[1]^T} = \frac{1}{m}\Delta^{[2]}A^{[1]^T} \\
& db^{[2]} = \frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True) 
\end{align}
\end{frame}
%..................................................................
\begin{frame}{Weights Update}
\textbf{Hidden Layer}
\begin{align}
dW^{[1]} &= \frac{1}{m} \left[ A^{[2]} - Y \right] \cdot  X^{T} \cdot W^{[2]T} \cdot (1 - A^{[1]^2}) \notag\\
         &= \frac{1}{m} \Delta^{[2]} \cdot W^{[2]T} \cdot (1 - A^{[1]^2}) \cdot  X^{T}   \notag\\
         &= \frac{1}{m} \Delta^{[1]} \cdot  X^{T} 
\end{align}
\begin{equation} 
db^{[1]} = \frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)  
\end{equation}
\end{frame}
%..................................................................
\begin{frame}{Example: Implementing a Shallow Neural Network}
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
        \begin{itemize}
		\item Using \textbf{03-introduction-to-deep-learning} Notebook 
		\item Par. 3.3
        \end{itemize}
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
    %\fbox{
        \includegraphics[width=\linewidth]{../../07-pictures/end_part_1.png}
    %}
\end{column}%
\end{columns}
\end{frame}
\subsection{Keras \\ \scalebox{0.8}{}}
%---------------------------------------------------------------------------------------------------
\begin{frame}{Introduction to keras}
	\begin{center}
	\includegraphics[scale=0.5]{../../07-pictures/03_intro_to_deep_learning_pic_21.png}
	\end{center}
	\begin{itemize}
		\item 0-9 handwritten digit recognition:
		\item MNIST Data maintained by Yann LeCun: http://yann.lecun.com/exdb/mnist/
		\item Keras provides data sets loading function at http://keras.io/datasets
	\end{itemize}
\end{frame}
%..................................................................
\begin{frame}{Implementing in Keras}
	Fully Connected NN
	\begin{center}
	\includegraphics[scale=0.6]{../../07-pictures/03_intro_to_deep_learning_pic_22.png}
	\end{center}
\end{frame}
%..................................................................
\begin{frame}{Implementing in Keras}
	Training
	\begin{center}
	\includegraphics[scale=0.4]{../../07-pictures/03_intro_to_deep_learning_pic_23.png}
	\end{center}
\end{frame}
%..................................................................
%=====================================================================


\end{document}
