{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7642c3e7",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecc891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70791adb",
   "metadata": {},
   "source": [
    "## What are Word Vectorization and Why It is so Important\n",
    "\n",
    "Word vectorization generically refers to techniques used to convert text into numbers. There may be different numerical representations of the same text. \n",
    "\n",
    "Many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing *strings* or *plain text* in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression, etc. in broad terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39baeec2",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1af1f",
   "metadata": {},
   "source": [
    "As we have already said, in order to perform machine learning on text, we need to transform our documents into vector representations such that we can apply numeric machine learning. This process is called feature extraction or more simply, vectorization.\n",
    "\n",
    "We will explore several choices, each of which extends or modifies the base bag-of-words model to describe semantic space. We will look at four types of vector encoding - frequency, one-hot, TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623ce0d",
   "metadata": {},
   "source": [
    "### Sample corpus of text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c9e83",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img align=\"left\" width=\"100\" height=\"100\" src=\"./img/text_analytics_with_python.jpg\"/> </td>\n",
    "<td> The following examples are taken from \"Text Analytics with Python\" by Dipanjan Sarkar (Apress, 2019).     </td>\n",
    "</tr></table>\n",
    "\n",
    "[Here](https://github.com/Apress/text-analytics-w-python-2e) you can find the original notebook used in the aforementioned textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d9b5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, e...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category\n",
       "0                     The sky is blue and beautiful.  weather\n",
       "1                  Love this blue and beautiful sky!  weather\n",
       "2       The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, e...     food\n",
       "4        I love green eggs, ham, sausages and bacon!     food\n",
       "5   The brown fox is quick and the blue dog is lazy!  animals\n",
       "6  The sky is very blue and the sky is very beaut...  weather\n",
       "7        The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7f982",
   "metadata": {},
   "source": [
    "### Simple text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd63a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0614c70",
   "metadata": {},
   "source": [
    "**Regular Expression Flags**\n",
    "\n",
    "**re.I - \n",
    "re.IGNORECASE**\n",
    "\n",
    "Perform case-insensitive matching; expressions like [A-Z] will also match lowercase letters. Full Unicode matching (such as Ü matching ü) also works unless the re.ASCII flag is used to disable non-ASCII matches. The current locale does not change the effect of this flag unless the re.LOCALE flag is also used. Corresponds to the inline flag (?i).\n",
    "\n",
    "**re.A - \n",
    "re.ASCII**\n",
    "\n",
    "Make \\\\w, \\\\W, \\\\b, \\\\B, \\\\d, \\\\D, \\\\s and \\\\S perform ASCII-only matching instead of full Unicode matching. This is only meaningful for Unicode patterns, and is ignored for byte patterns. Corresponds to the inline flag (?a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963c9d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb0a99",
   "metadata": {},
   "source": [
    "<!--\n",
    "![image.png](./img/3_1_text_vectorization_pic_0.png)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890dd823",
   "metadata": {},
   "source": [
    "### Frequency Vectors\n",
    "\n",
    "The simplest vector encoding model is to simply fill in the vector with the frequency of each word as it appears in the document;\n",
    "In this encoding scheme each document is represented as the multiset of the tokens that compose it and the value for each word position in the vectr is its count;\n",
    "This representation can either be a straight count encoding or a normalized encoding where each word is weighted by the total number of words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092ad55",
   "metadata": {},
   "source": [
    "In **Scikit-Learn** The CountVectorizer transformer from the sklearn.feature_extraction model has its own internal tokenization and normalization methods. The fit method of the vectorizer expects an iterable or list of strings or file objects, and creates a dictionary of the vocabulary on the corpus. When transform is called, each individual document is transformed into a sparse array whose index tuple is the row (the document ID) and the token ID from the dictionary, and whose value is the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e38c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baccd31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70d2ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown  dog  eggs  fox  green  \\\n",
       "0      0      0          1     1          0      0    0     0    0      0   \n",
       "1      0      0          1     1          0      0    0     0    0      0   \n",
       "2      0      0          0     0          0      1    1     0    1      0   \n",
       "3      1      1          0     0          1      0    0     1    0      0   \n",
       "4      1      0          0     0          0      0    0     1    0      1   \n",
       "5      0      0          0     1          0      1    1     0    1      0   \n",
       "6      0      0          1     1          0      0    0     0    0      0   \n",
       "7      0      0          0     0          0      1    1     0    1      0   \n",
       "\n",
       "   ham  jumps  kings  lazy  love  quick  sausages  sky  toast  today  \n",
       "0    0      0      0     0     0      0         0    1      0      0  \n",
       "1    0      0      0     0     1      0         0    1      0      0  \n",
       "2    0      1      0     1     0      1         0    0      0      0  \n",
       "3    1      0      1     0     0      0         1    0      1      0  \n",
       "4    1      0      0     0     1      0         1    0      0      0  \n",
       "5    0      0      0     1     0      1         0    0      0      0  \n",
       "6    0      0      0     0     0      0         0    2      0      1  \n",
       "7    0      0      0     1     0      1         0    0      0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad73797",
   "metadata": {},
   "source": [
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "Because they disregard grammar and the relative position of words in documents, frequency-based encoding methods suffer from the long tail, or Zipfian distribution, that characterizes natural language. As a result, tokens that occur very frequently are orders of magnitude more “significant” than other, less frequent ones. This can have a significant impact on some models (e.g., generalized linear models) that expect normally distributed features. \n",
    "\n",
    "A solution to this problem is **one-hot encoding**, a boolean vector encoding method that marks a particular vector index with a value of true (1) if the token exists in the document and false (0) if it does not. In other words, each element of a one-hot encoded vector reflects either the presence or absence of the token in the described text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "384d7da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1., binary=True)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b97e877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b67cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**TF - Term Frequency**: Just counting the number of words in each document has 1 issue: it will give more weightage to longer documents than shorter documents. To avoid this, we can use frequency i.e. #count(word) / #Total words, in each document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3cb15",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency\n",
    "\n",
    "The bag-of-words representations that we have explored so far only describe a document in a standalone fashion, not taking into account the context of the corpus. \n",
    "A better approach would be to consider the relative frequency or rareness of tokens in the document against their frequency in other documents. The central insight is that meaning is most likely encoded in the more rare terms from a document.\n",
    "\n",
    "TF-IDF, term frequency-inverse document frequency, encoding normalizes the frequency of tokens in a document with respect to the rest of the corpus. \n",
    "This encoding approach accentuates terms that are very relevant to a specific instance, as shown in Figure, where the token studio has a higher relevance to this document since it only appears there.\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. \n",
    "It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient: \n",
    "\n",
    "\\begin{equation}idf(t,D) = \\log \\frac{N}{\\vert \\{  d \\in D: t \\in d\\}\\vert} \\end{equation}  \n",
    "\n",
    "where the numerator ($N$) is the total number of documents in the corpus and the denominator is the number of documents where the term $t$ appears.\n",
    "\n",
    "Then tf-idf is calculated as: \n",
    "\n",
    "\\begin{equation} tfidf(t,d,D) = tf(t,d) \\cdot idf(t,D)\\end{equation}\n",
    "\n",
    "A high weight in tf-idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms.  Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a3b0b",
   "metadata": {},
   "source": [
    "![image.png](./img/tf-idf-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344bcc72",
   "metadata": {},
   "source": [
    "A simple example might serve to explain the structure of the TDM more clearly. Assume we have a simple corpus consisting of two documents, Doc1 and Doc2, with the following content:\n",
    "\n",
    "Doc1 = \"I like databases\"\n",
    "Doc2 = \"I dislike databases\",\n",
    "\n",
    "then the document-term matrix would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668d879",
   "metadata": {},
   "source": [
    "![image.png](./img/tf-idf-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccfa1ff",
   "metadata": {},
   "source": [
    "Clearly there is nothing special about rows and columns – we could just as easily transpose them. If we did so, we’d get a term document matrix (TDM) in which the terms are rows and documents columns. One can work with either a DTM or TDM. Using the raw count of a term in a document, i.e. the number of times that term t occurs in document d, is the simplest choice to measure the term frequency $tf(t,d)$. If we denote the raw count by $f_{t,d}$, then the simplest $tf$ scheme is $tf(t,d) = f_{t,d}$. Other possibilities include\n",
    "\n",
    "- Boolean \"frequencies\": $tf(t,d) = 1$ if $t$ occurs in $d$ and $0$ otherwise;\n",
    "- Term frequency adjusted for document length : $f_{t,d} \\big/ \\text{(number of words in d)}$;\n",
    "- Logarithmically scaled frequency: $tf(t,d) = \\log (1 + f_{t,d})$;\n",
    "- Augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the raw frequency of the most occurring term in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1cfa05",
   "metadata": {},
   "source": [
    "![image.png](./img/3_1_text_vectorization_pic_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef75c7",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a transformer called the TfidfVectorizer in the module called **feature_extraction.text** for vectorizing documents with TF–IDF scores. Under the hood, the TfidfVectorizer uses the CountVectorizer estimator we used to produce the bag-of-words encoding to count occurrences of tokens, followed by a TfidfTransformer, which normalizes these occurrence counts by the inverse document frequency. \n",
    "\n",
    "The input for a TfidfVectorizer is expected to be a sequence of filenames, file-like objects, or strings that contain a collection of raw documents, similar to that of the CountVectorizer. As a result, a default tokenization and preprocessing method is applied unless other functions are specified. The vectorizer returns a sparse matrix representation in the form of ((doc, term), tfidf) where each key is a document and term pair and the value is the TF–IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61a5b843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n",
       "0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n",
       "3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n",
       "4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n",
       "5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n",
       "6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n",
       "\n",
       "    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n",
       "1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n",
       "2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n",
       "3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n",
       "4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n",
       "5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n",
       "6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n",
       "7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2',\n",
    "                     use_idf=True, smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14054a",
   "metadata": {},
   "source": [
    "### Extracting Features for New Documents\n",
    "\n",
    "Suppose you built a machine learning model to classify and categorize news articles and it is currently in production. How you can generate features for completely new documents so that you can feed it into the machine learning models for prediction? The Scikit-Learn API provides the `transform` function for the vectorizers we discussed previously and we can leverage it to ge features for a completely new document that was not present in our corpus. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73276e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown  dog  eggs  fox  green  \\\n",
       "0    0.0    0.0        0.0   0.0        0.0    0.0  0.0   0.0  0.0   0.63   \n",
       "\n",
       "   ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.0    0.0    0.0   0.0   0.0    0.0       0.0  0.46    0.0   0.63  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = 'the sky is green today'\n",
    "\n",
    "pd.DataFrame(np.round(tv.transform([new_doc]).toarray(), 2), \n",
    "             columns=tv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58868fea",
   "metadata": {},
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4e2a5",
   "metadata": {},
   "source": [
    "When you have vectorized your text, we can try to define a distance metric such that documents that are closer together in feature space are more similar. There are a number of different measures that can be used to determine document similarity. Fundamentally, each relies on our ability to imagine documents as points in space, where the relative closeness of any two documents is a measure of their similarity.\n",
    "\n",
    "We can measure vector similarity with cosine distance, using the cosine of the angle between the two vectors to assess the degree to which they share the same orientation. In effect, the more parallel any two vectors are, the more similar the documents will be (regardless of their magnitude)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fad09",
   "metadata": {},
   "source": [
    "<!--\n",
    "<div>\n",
    "<img src=\"./img/3_1_text_vectorization_pic_9.png\" width=\"500\"/>\n",
    "</div>\n",
    "-->\n",
    "![pic](./img/3_1_text_vectorization_pic_9.png)\n",
    "\n",
    "*(image source: Bengfort B. et al. \"Text Analysis with Python\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0696c72",
   "metadata": {},
   "source": [
    "Mathematically, Cosine similarity metric measures the cosine of the angle between two n-dimensional vectors projected in a multi-dimensional space. The Cosine similarity of two documents will range from 0 to 1. If the Cosine similarity score is 1, it means two vectors have the same orientation. The value closer to 0 indicates that the two documents have less similarity.\n",
    "\n",
    "The mathematical equation of Cosine similarity between two non-zero vectors is: \n",
    "\n",
    "\\begin{equation} \\text{similarity} = \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\left\\Vert\\mathbf{A}\\right\\Vert \\,\\left\\Vert\\mathbf{B}\\right\\Vert} = \\frac{\\sum\\limits_{i=1}^n A_iB_i}{\\sqrt{\\sum\\limits_{i=1}^n A_i^2} \\sqrt{ \\sum\\limits_{i=1}^n B_i^2}} \\end{equation}\n",
    "\n",
    "Let’s see an example of how to calculate the cosine similarity between two text document. The common way to compute the Cosine similarity is to first we need to count the word occurrence in each document. To count the word occurrence in each document, we can use **CountVectorizer** or **TfidfVectorizer** functions that are provided by Scikit-Learn library.\n",
    "\n",
    "doc_1 = \"Data is the oil of the digital economy\" \n",
    "doc_2 = \"Data is a new oil\" \n",
    "\n",
    "and consider the following frequency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45755e63",
   "metadata": {},
   "source": [
    "<!--\n",
    "<img src='./img/tf-idf-2.png'>\n",
    "-->\n",
    "![image.png](./img/tf-idf-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b514f5",
   "metadata": {},
   "source": [
    "We can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "982fc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = \"Data is the oil of the digital economy\" \n",
    "doc_2 = \"Data is a new oil\" \n",
    "data = [doc_1, doc_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "850c2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "vector_matrix = count_vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d0818",
   "metadata": {},
   "source": [
    "Here, is the unique tokens list found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d99a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'digital', 'economy', 'is', 'new', 'of', 'oil', 'the']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = count_vectorizer.get_feature_names()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c0b206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 1, 1, 2],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c08bbe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>digital</th>\n",
       "      <th>economy</th>\n",
       "      <th>is</th>\n",
       "      <th>new</th>\n",
       "      <th>of</th>\n",
       "      <th>oil</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       data  digital  economy  is  new  of  oil  the\n",
       "doc_1     1        1        1   1    0   1    1    2\n",
       "doc_2     1        0        0   1    1   0    1    0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataframe(matrix, tokens):\n",
    "\n",
    "    doc_names = [f'doc_{i+1}' for i, _ in enumerate(matrix)]\n",
    "    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)\n",
    "    return(df)\n",
    "\n",
    "create_dataframe(vector_matrix.toarray(),tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1d78df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vector_matrix.toarray()\n",
    "v1 = np.array(a[0])\n",
    "v2 = np.array(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "651b00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.linalg.norm(v1)\n",
    "n2 = np.linalg.norm(v2)\n",
    "prod = np.inner(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7450eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4743416490252569\n"
     ]
    }
   ],
   "source": [
    "print(prod/(n1*n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edf97f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.474342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.474342</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc_1     doc_2\n",
       "doc_1  1.000000  0.474342\n",
       "doc_2  0.474342  1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd9ca9",
   "metadata": {},
   "source": [
    "Let’s check the cosine similarity with `TfidfVectorizer`, and see how it change over `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f05c0d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>digital</th>\n",
       "      <th>economy</th>\n",
       "      <th>is</th>\n",
       "      <th>new</th>\n",
       "      <th>of</th>\n",
       "      <th>oil</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>0.243777</td>\n",
       "      <td>0.34262</td>\n",
       "      <td>0.34262</td>\n",
       "      <td>0.243777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34262</td>\n",
       "      <td>0.243777</td>\n",
       "      <td>0.68524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.630099</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           data  digital  economy        is       new       of       oil  \\\n",
       "doc_1  0.243777  0.34262  0.34262  0.243777  0.000000  0.34262  0.243777   \n",
       "doc_2  0.448321  0.00000  0.00000  0.448321  0.630099  0.00000  0.448321   \n",
       "\n",
       "           the  \n",
       "doc_1  0.68524  \n",
       "doc_2  0.00000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer()\n",
    "vector_matrix = Tfidf_vect.fit_transform(data)\n",
    "\n",
    "tokens = Tfidf_vect.get_feature_names()\n",
    "create_dataframe(vector_matrix.toarray(),tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b92ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_1</th>\n",
       "      <th>doc_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.327871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_2</th>\n",
       "      <td>0.327871</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc_1     doc_2\n",
       "doc_1  1.000000  0.327871\n",
       "doc_2  0.327871  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967774d5",
   "metadata": {},
   "source": [
    "Here, using `TfidfVectorizer` we get the cosine similarity between doc_1 and doc_2 is 0.32.  Where the `CountVectorizer` has returned the cosine similarity of doc_1 and doc_2 is 0.47. This is because `TfidfVectorizer` penalized the most frequent words in the document such as stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859ced2",
   "metadata": {},
   "source": [
    "Now, the **distance** can be defined as \n",
    "\n",
    "\\begin{equation}\n",
    "d =1-\\mathrm{CosineSimilarity} \n",
    "\\end{equation}\n",
    "\n",
    "The intuition behind this is that if 2 vectors are perfectly the same then similarity is 1 (angle=0) and thus, distance is 0 (1-1=0).\n",
    "\n",
    "\n",
    "Let's apply the same analysis to our toy corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcd759d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192353</td>\n",
       "      <td>0.817246</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.820599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225489</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.670631</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.192353</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.791821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115488</td>\n",
       "      <td>0.930989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.817246</td>\n",
       "      <td>0.670631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.000000  0.820599  0.000000  0.000000  0.000000  0.192353  0.817246   \n",
       "1  0.820599  1.000000  0.000000  0.000000  0.225489  0.157845  0.670631   \n",
       "2  0.000000  0.000000  1.000000  0.000000  0.000000  0.791821  0.000000   \n",
       "3  0.000000  0.000000  0.000000  1.000000  0.506866  0.000000  0.000000   \n",
       "4  0.000000  0.225489  0.000000  0.506866  1.000000  0.000000  0.000000   \n",
       "5  0.192353  0.157845  0.791821  0.000000  0.000000  1.000000  0.115488   \n",
       "6  0.817246  0.670631  0.000000  0.000000  0.000000  0.115488  1.000000   \n",
       "7  0.000000  0.000000  0.850516  0.000000  0.000000  0.930989  0.000000   \n",
       "\n",
       "          7  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.850516  \n",
       "3  0.000000  \n",
       "4  0.000000  \n",
       "5  0.930989  \n",
       "6  0.000000  \n",
       "7  1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f1104",
   "metadata": {},
   "source": [
    "**Clustering documents using similarity features**\n",
    "\n",
    "If you want a very nice introduction to hierarchical clustering see [this](https://towardsdatascience.com/hierarchical-clustering-explained-e59b13846da8) post. In the beginning of the agglomerative clustering process, each element is in a cluster of its own. The clusters are then sequentially combined into larger clusters, until all elements end up being in the same cluster. At each step, the two clusters separated by the shortest distance are combined. The function used to determine the distance between two clusters, known as the linkage function, is what differentiates the agglomerative clustering methods. In our case we use as linkage the similarity_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc36067d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document\\Cluster 1</th>\n",
       "      <th>Document\\Cluster 2</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Cluster Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.253098</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.308539</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.386952</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.489845</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.732945</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2.69565</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3.45108</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document\\Cluster 1 Document\\Cluster 2  Distance Cluster Size\n",
       "0                  2                  7  0.253098            2\n",
       "1                  0                  6  0.308539            2\n",
       "2                  5                  8  0.386952            3\n",
       "3                  1                  9  0.489845            3\n",
       "4                  3                  4  0.732945            2\n",
       "5                 11                 12   2.69565            5\n",
       "6                 10                 13   3.45108            8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(similarity_matrix, 'ward')\n",
    "pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2', \n",
    "                         'Distance', 'Cluster Size'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e7e8164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1dff8a90b88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Data point')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(Z)\n",
    "plt.axhline(y=1.0, c='k', ls='--', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b161c269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, e...</td>\n",
       "      <td>food</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category  ClusterLabel\n",
       "0                     The sky is blue and beautiful.  weather             2\n",
       "1                  Love this blue and beautiful sky!  weather             2\n",
       "2       The quick brown fox jumps over the lazy dog.  animals             1\n",
       "3  A king's breakfast has sausages, ham, bacon, e...     food             3\n",
       "4        I love green eggs, ham, sausages and bacon!     food             3\n",
       "5   The brown fox is quick and the blue dog is lazy!  animals             1\n",
       "6  The sky is very blue and the sky is very beaut...  weather             2\n",
       "7        The dog is lazy but the brown fox is quick!  animals             1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_dist = 1.0\n",
    "\n",
    "cluster_labels = fcluster(Z, max_dist, criterion='distance')\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3ee4a",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6bb2c",
   "metadata": {},
   "source": [
    "Humans have always excelled at understanding languages. It is easy for humans to understand the relationship between words but for computers, this task may not be simple. Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. For example, we humans understand the words like king and queen, man and woman, tiger and tigress have a certain type of relation between them but how can a computer figure this out?\n",
    "\n",
    "The different encoding we have discussed so far is arbitrary as it does not capture any relationship between words. \n",
    "It can be challenging for a model to interpret, for example, a linear classifier learns a single weight for each feature. \n",
    "Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
    "\n",
    "It should be nice to have representations of text in an n-dimensional space where words that have the same meaning have a similar representation. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space. \n",
    "\n",
    "Thus when using word embeddings, all individual words are represented as real-valued vectors in a predefined vector space. \n",
    "Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network. Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n",
    "\n",
    "The concept of embeddings arises from a branch of Natural Language Processing called - “Distributional Semantics”. It is based on the simple intuition that: ***Words that occur in similar contexts tend to have similar meanings***. In other words, a word’s meaning is given by the words that it appears frequently with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9d50b",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92315c13",
   "metadata": {},
   "source": [
    "Word2vec is a method to efficiently create word embeddings by using a two-layer neural network.  It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the embedding more efficient and since then has become the de facto standard for developing pre-trained word embedding. \n",
    "\n",
    "The input of word2vec is a text corpus and its output is a set of vectors known as feature vectors that represent words in that corpus. The Word2Vec objective function causes the words that have a similar context to have similar embeddings. \n",
    "Thus in this vector space, these words are really close. Mathematically, the cosine of the angle (Q) between such vectors should be close to 1, i.e. angle close to 0.\n",
    "\n",
    "Word2vec is not a single algorithm but a combination of two techniques - **CBOW(Continuous bag of words)** and **Skip-gram** model. Both these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both these techniques learn weights which act as word vector representations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d899ea0",
   "metadata": {},
   "source": [
    "### CBOW Architecture \n",
    "\n",
    "CBOW predicts the probability of a word to occur **given the words surrounding it**. We can consider a single word or a group of words. But for simplicity, we will take a single context word and try to predict a single target word.\n",
    "The English language contains almost 1.2 million words, making it impossible to include so many words in our example. So we will consider a small example in which we have only four words i.e. ***live***, ***home***, ***they*** and ***at***. For simplicity, we will consider that the corpus contains only one sentence, that being, ***They live at home***. First, we convert each word into a one-hot encoding form. Also, we'll not consider all the words in the sentence but ll only take certain words that are in a window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f6605",
   "metadata": {},
   "source": [
    "![image.png](./img/word-embedding-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d9dea",
   "metadata": {},
   "source": [
    "For example for a window size equal to three, we only consider three words\n",
    "in a sentence. The middle word is to be predicted and the surrounding two\n",
    "words are fed into the neural network as context. The window is then slid\n",
    "and the process is repeated again. Finally, after training the network repeatedly by sliding the window a\n",
    "shown above, we get weights which we use to get the embeddings as\n",
    "shown below. Usually, we take a window size of around 8-10 words and have a\n",
    "vector size of 300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b585ff",
   "metadata": {},
   "source": [
    "![image.png](./img/word-embedding-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c585926",
   "metadata": {},
   "source": [
    "### Skip-gram model\n",
    "\n",
    "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the centre word)\n",
    "\n",
    "The working of the skip-gram model is quite similar to the CBOW but there is just a difference in the architecture of its neural network and the way the weight matrix is generated  as shown in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bac1d",
   "metadata": {},
   "source": [
    "![image.png](./img/3_1_text_vectorization_pic_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41294b",
   "metadata": {},
   "source": [
    "So now which one of the two algorithms should we use for implementing word2vec? Turns out for large corpus with higher dimensions, it is better to use skip-gram but is slow to train. Whereas CBOW is better for small corpus and is faster to train too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd92e5",
   "metadata": {},
   "source": [
    "## Implementing a word2vec model using a CBOW NN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfc1f4",
   "metadata": {},
   "source": [
    "### Load up sample corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "254a8611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19f28544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "book = gutenberg.sents('carroll-alice.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e5a7e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 1703\n",
      "\n",
      "Sample line: ['The', 'rabbit', '-', 'hole', 'went', 'straight', 'on', 'like', 'a', 'tunnel', 'for', 'some', 'way', ',', 'and', 'then', 'dipped', 'suddenly', 'down', ',', 'so', 'suddenly', 'that', 'Alice', 'had', 'not', 'a', 'moment', 'to', 'think', 'about', 'stopping', 'herself', 'before', 'she', 'found', 'herself', 'falling', 'down', 'a', 'very', 'deep', 'well', '.']\n",
      "\n",
      "Processed line: thought alice fall shall think nothing tumbling stairs\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "remove_terms = punctuation + '0123456789'\n",
    "\n",
    "norm_book = [[word.lower() for word in sent if word not in remove_terms] for sent in book]\n",
    "norm_book = [' '.join(tok_sent) for tok_sent in norm_book]\n",
    "norm_book = filter(None, normalize_corpus(norm_book))\n",
    "norm_book = [tok_sent for tok_sent in norm_book if len(tok_sent.split()) > 2]\n",
    "\n",
    "print('Total lines:', len(book))\n",
    "print('\\nSample line:', book[10])\n",
    "print('\\nProcessed line:', norm_book[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f458522d",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "458ac174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils         import np_utils\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_book)\n",
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6f83d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2379\n",
      "Vocabulary Sample: [('said', 1), ('alice', 2), ('little', 3), ('one', 4), ('would', 5), ('know', 6), ('went', 7), ('like', 8), ('could', 9), ('time', 10)]\n"
     ]
    }
   ],
   "source": [
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_book]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e696ca8",
   "metadata": {},
   "source": [
    "### Build (context_words, target_word) pair generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ed016ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec91befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['alice', 'adventures', 'lewis', 'carroll'] -> Target (Y): wonderland\n",
      "Context (X): ['alice', 'beginning', 'tired', 'sitting'] -> Target (Y): get\n",
      "Context (X): ['beginning', 'get', 'sitting', 'sister'] -> Target (Y): tired\n",
      "Context (X): ['get', 'tired', 'sister', 'bank'] -> Target (Y): sitting\n",
      "Context (X): ['tired', 'sitting', 'bank', 'nothing'] -> Target (Y): sister\n",
      "Context (X): ['sitting', 'sister', 'nothing', 'twice'] -> Target (Y): bank\n",
      "Context (X): ['sister', 'bank', 'twice', 'peeped'] -> Target (Y): nothing\n",
      "Context (X): ['bank', 'nothing', 'peeped', 'book'] -> Target (Y): twice\n",
      "Context (X): ['nothing', 'twice', 'book', 'sister'] -> Target (Y): peeped\n",
      "Context (X): ['twice', 'peeped', 'sister', 'reading'] -> Target (Y): book\n",
      "Context (X): ['peeped', 'book', 'reading', 'pictures'] -> Target (Y): sister\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2e414",
   "metadata": {},
   "source": [
    "### Build CBOW Deep Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7682dead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 100)            237900    \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2379)              240279    \n",
      "=================================================================\n",
      "Total params: 478,179\n",
      "Trainable params: 478,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d0b62",
   "metadata": {},
   "source": [
    "### Train model for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3decc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 1 \tLoss: 82861.53140425682\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 2 \tLoss: 85602.46584838629\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 3 \tLoss: 97885.54514807463\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 4 \tLoss: 101883.65546479821\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 5 \tLoss: 102871.60655751824\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 6 \tLoss: 102841.33749639988\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 7 \tLoss: 101846.42558838427\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 8 \tLoss: 101338.09550878406\n",
      "\n",
      "Processed 10000 (context, word) pairs\n",
      "Epoch: 9 \tLoss: 100069.8285932988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08842eb1",
   "metadata": {},
   "source": [
    "### Get word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8da56e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2378, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alice</th>\n",
       "      <td>0.110740</td>\n",
       "      <td>-0.206482</td>\n",
       "      <td>-0.095507</td>\n",
       "      <td>0.125904</td>\n",
       "      <td>-0.029820</td>\n",
       "      <td>-1.373520</td>\n",
       "      <td>-0.533391</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>-0.231086</td>\n",
       "      <td>-0.001672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.338460</td>\n",
       "      <td>0.120766</td>\n",
       "      <td>0.133433</td>\n",
       "      <td>0.026742</td>\n",
       "      <td>-0.166425</td>\n",
       "      <td>-0.121798</td>\n",
       "      <td>0.218291</td>\n",
       "      <td>0.033906</td>\n",
       "      <td>-0.325388</td>\n",
       "      <td>-0.006172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>-0.111091</td>\n",
       "      <td>-0.026161</td>\n",
       "      <td>0.157881</td>\n",
       "      <td>0.021539</td>\n",
       "      <td>-0.588673</td>\n",
       "      <td>-0.183460</td>\n",
       "      <td>0.053935</td>\n",
       "      <td>-0.055449</td>\n",
       "      <td>0.145096</td>\n",
       "      <td>-0.075216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166039</td>\n",
       "      <td>0.164547</td>\n",
       "      <td>-0.290646</td>\n",
       "      <td>0.612585</td>\n",
       "      <td>-0.194902</td>\n",
       "      <td>-0.252110</td>\n",
       "      <td>0.416965</td>\n",
       "      <td>-0.039088</td>\n",
       "      <td>0.186179</td>\n",
       "      <td>0.207721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.092267</td>\n",
       "      <td>0.243680</td>\n",
       "      <td>0.273080</td>\n",
       "      <td>-0.004214</td>\n",
       "      <td>-0.253085</td>\n",
       "      <td>-0.229521</td>\n",
       "      <td>0.300807</td>\n",
       "      <td>0.559803</td>\n",
       "      <td>0.446665</td>\n",
       "      <td>0.045866</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224569</td>\n",
       "      <td>0.327376</td>\n",
       "      <td>-0.244948</td>\n",
       "      <td>0.063515</td>\n",
       "      <td>-0.054172</td>\n",
       "      <td>0.128954</td>\n",
       "      <td>0.281234</td>\n",
       "      <td>-0.110221</td>\n",
       "      <td>-0.018703</td>\n",
       "      <td>0.033904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>-0.016690</td>\n",
       "      <td>0.219785</td>\n",
       "      <td>-0.117265</td>\n",
       "      <td>0.164920</td>\n",
       "      <td>0.142406</td>\n",
       "      <td>-0.069837</td>\n",
       "      <td>-0.463099</td>\n",
       "      <td>-0.174782</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137855</td>\n",
       "      <td>0.098561</td>\n",
       "      <td>0.117097</td>\n",
       "      <td>-0.186808</td>\n",
       "      <td>-0.165801</td>\n",
       "      <td>0.095544</td>\n",
       "      <td>0.120867</td>\n",
       "      <td>-0.008317</td>\n",
       "      <td>0.060949</td>\n",
       "      <td>-0.023347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>0.093930</td>\n",
       "      <td>-0.368835</td>\n",
       "      <td>0.164864</td>\n",
       "      <td>-0.094852</td>\n",
       "      <td>-0.297923</td>\n",
       "      <td>-0.152862</td>\n",
       "      <td>-0.170664</td>\n",
       "      <td>0.263197</td>\n",
       "      <td>0.260616</td>\n",
       "      <td>0.122768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.497493</td>\n",
       "      <td>0.057678</td>\n",
       "      <td>-0.190825</td>\n",
       "      <td>-0.027801</td>\n",
       "      <td>-0.130807</td>\n",
       "      <td>0.189335</td>\n",
       "      <td>0.051051</td>\n",
       "      <td>-0.453787</td>\n",
       "      <td>0.199254</td>\n",
       "      <td>-0.255743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "alice   0.110740 -0.206482 -0.095507  0.125904 -0.029820 -1.373520 -0.533391   \n",
       "little -0.111091 -0.026161  0.157881  0.021539 -0.588673 -0.183460  0.053935   \n",
       "one     0.092267  0.243680  0.273080 -0.004214 -0.253085 -0.229521  0.300807   \n",
       "would  -0.016690  0.219785 -0.117265  0.164920  0.142406 -0.069837 -0.463099   \n",
       "know    0.093930 -0.368835  0.164864 -0.094852 -0.297923 -0.152862 -0.170664   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "alice   0.057471 -0.231086 -0.001672  ... -0.338460  0.120766  0.133433   \n",
       "little -0.055449  0.145096 -0.075216  ... -0.166039  0.164547 -0.290646   \n",
       "one     0.559803  0.446665  0.045866  ... -0.224569  0.327376 -0.244948   \n",
       "would  -0.174782  0.003315  0.014213  ... -0.137855  0.098561  0.117097   \n",
       "know    0.263197  0.260616  0.122768  ... -0.497493  0.057678 -0.190825   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "alice   0.026742 -0.166425 -0.121798  0.218291  0.033906 -0.325388 -0.006172  \n",
       "little  0.612585 -0.194902 -0.252110  0.416965 -0.039088  0.186179  0.207721  \n",
       "one     0.063515 -0.054172  0.128954  0.281234 -0.110221 -0.018703  0.033904  \n",
       "would  -0.186808 -0.165801  0.095544  0.120867 -0.008317  0.060949 -0.023347  \n",
       "know   -0.027801 -0.130807  0.189335  0.051051 -0.453787  0.199254 -0.255743  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34dd72",
   "metadata": {},
   "source": [
    "### Build a distance matrix to view the most similar words (contextually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf1b07f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2378, 2378)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cat': ['cheshire', 'mad', 'raving', 'conversation', 'live'],\n",
       " 'caterpillar': ['eggs', 'pigeon', 'mushroom', 'hookah', 'held'],\n",
       " 'hatter': ['bread', 'butter', 'teacup', 'jurors', 'officers'],\n",
       " 'queen': ['three', 'hedgehog', 'arm', 'spoke', 'makes']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['cat', 'caterpillar', 'hatter', 'queen']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582c914",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bb2c6",
   "metadata": {},
   "source": [
    "As we have said, word vectors techniques put words to a vector space, where similar words cluster together and different words repel. The advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words), but incorporates global statistics (word co-occurrence) to obtain word vectors. In general there is quite a bit of synergy between the GloVe and Word2vec.\n",
    "\n",
    "#### Co-Occurence Matrix\n",
    "\n",
    "Generally speaking, a co-occurrence matrix will have specific entities in rows (ER) and columns (EC). The purpose of this matrix is to present the number of times each ER appears in the same context as each EC. As a consequence, in order to use a co-occurrence matrix, you have to define your entites and the context in which they co-occur. In NLP, the most classic approach is to define each entity (ie, lines and columns) as a word present in a text, and the context as a sentence or in the $\\pm n$ word window - depends on the application.\n",
    "\n",
    "Similar words tend to occur together and will have a similar context for example : Apple is a fruit. Mango is a fruit.\n",
    "Apple and mango tend to have a similar context i.e fruit. Co-occurrence : For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window. Context Window : Context window is specified by a number and the direction.\n",
    "\n",
    "The matrix A stores co-occurrences of words. In this method, we count the number of times each word appears inside a window of a particular size around the word of interest. Calculate this count for all the words in the corpus. Let us understand all of this with the help of an example.\n",
    "\n",
    "Let our corpus contain the following three sentences:\n",
    "\n",
    "- I enjoy flying\n",
    "- I like NLP\n",
    "- I like deep learning\n",
    "\n",
    "Let ***window size = 1***. This means that context words for each and every word are ***1 word to the left and one to the right***.\n",
    "\n",
    "Context words for:\n",
    "\n",
    "- I = enjoy(1 time), like(2 times)\n",
    "- enjoy = I (1 time), flying(2 times)\n",
    "- flying = enjoy(1 time)\n",
    "- like = I(2 times), NLP(1 time), deep(1 time)\n",
    "- NLP = like(1 time)\n",
    "- deep = like(1 time), learning(1 time)\n",
    "- learning = deep(1 time)\n",
    "\n",
    "Therefore, the resultant co-occurrence matrix A with fixed window size 1 looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040c897",
   "metadata": {},
   "source": [
    "![image.png](./img/3_1_text_vectorization_pic_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c0113",
   "metadata": {},
   "source": [
    "So, from a formal point of view, given a corpus having $V$ words, the co-occurrence matrix $X$ will be a $V  \\times V$ matrix, where the $i$-th row and $j$-th column of $X$,$X_{ij}$ denotes how many times word $i$ has co-occurred with word $j$\n",
    "\n",
    "The number of “contexts” is, of course, large, since it is essentially combinatorial in size. So then we factorize this matrix to yield a lower-dimensional matrix, where each row now yields a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data.\n",
    "\n",
    "From the matrix elements we can define the quantity $P_{ik}$ as the probability of seeing word $i$ and $k$ together, this is computed by dividing the number of times $i$ and $k$ appeared together ($X_{ik}$) by the total number of times word $i$ appeared in the corpus ($X_i$). Consider now the ratio \\begin{equation} \\frac{P_{ik}}{P_{jk}} \\end{equation} where $k$ is another word in the text. For example, let's take a situation reported in the original article regarding words like *water*, *steam* and *ice* together with *solid* and *liquid* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645af759",
   "metadata": {},
   "source": [
    "![image.png](./img/3_1_text_vectorization_pic_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d08d132",
   "metadata": {},
   "source": [
    "You can see that given two words, i.e. ice and steam, if the third word $k$ (also called the *probe word*):\n",
    "\n",
    "- is very similar to ice but irrelevant to steam (e.g. k=solid), $P_{ik}/P_{jk}$ will be **very high (> 1)**,\n",
    "- is very similar to steam but irrelevant to ice (e.g. k=gas), $P_{ik}/P_{jk}$ will be **very small (< 1)**;\n",
    "- is related or unrelated to either words, then $P_{ik}/P_{jk}$ **will be close to 1**.\n",
    "\n",
    "So, if we can find a way to incorporate $P_{ik}/P_{jk}$ to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.\n",
    "\n",
    "Assume that there is a function $F$ which takes in word vectors of $i,j$ and $k$ which outputs the ratio we are interested in: \n",
    "\n",
    "\\begin{equation}F(w_i,w_j, u_k) = \\frac{P_{ik}}{P_{jk}} \\end{equation}\n",
    "\n",
    "Word vectors are linear systems. For example, you should perform arithmetic in embedding space, e.g.: \n",
    "\n",
    "\\begin{equation} \\mathbf{w_{king} - w_{male} + w_{female} = w_{queen}}\\end{equation}\n",
    "\n",
    "Therefore, let us change the above equation to the following: \n",
    "\n",
    "\\begin{equation} F(w_i - w_j, u_k) = \\frac{P_{ik}}{P_{jk}} \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51486ad5",
   "metadata": {},
   "source": [
    "**Vector to a scalar**\n",
    "\n",
    "How do we make LHS a scalar? There is a pretty straight forward answer to this. That is to introduce a transpose and a dot product between the two entities the following way: \n",
    "\n",
    "\\begin{equation} F((w_i - w_j)^T \\cdot u_k) = P_{ik}/P_{jk} \\end{equation}\n",
    "\n",
    "If you assume a word vector as a $D\\times 1$ matrix, $(w_i - w_j)^T$ will be $1\\times D$ shaped which gives a scalar when multiplied with $u_k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf285d67",
   "metadata": {},
   "source": [
    "**Finding the final equation**\n",
    "\n",
    "Next, if we assume $F$ has a certain property (i.e. homomorphism between additive group and the multiplicative group) which gives,\n",
    "\n",
    "$$F(w_i^T u_k-w_j^T u_k) = \\frac{F(w_i^T u_k)}{F(w_j^T u_k)} = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "In other words this particular homomorphism ensures that the subtraction $F(A-B)$ can also be represented as a division $F(A)/F(B)$ and get the same result. \n",
    "\n",
    "Note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles. To do so consistently we must not only exchange $w \\leftrightarrow u$ but also $X \\leftrightarrow X^T$. Our final model is invariant under this relabeling, this is another reason for the particular choice of the homomorphism.\n",
    "\n",
    "And therefore,\n",
    "\n",
    "$$\\frac{F(w_i^T u_k)}{F(w_j^T u_k)} = \\frac{P_{ik}}{P_{jk}} \\Rightarrow F(w_i^T u_k) = P_{ik}$$\n",
    "\n",
    "If we assume F=exp the above homomorphism property is satisfied. Then let us set,\n",
    "\n",
    "$$\\exp(w_i^T u_k) = P_{ik}= \\frac{X_{ik}}{X_i}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$w_i^T u_k = log(X_{ik})-log(X_i)$$\n",
    "\n",
    "Next we note that this equation would exhibit the exchange symmetry if not for the $log(X_i)$ on the right hand side. However this term is independent on $k$ so it can be absorbed into a bias $b_i$ for $w_i$. Finally adding and addictional bias $b_k$ for $u_k$ restores the symmetry,\n",
    "\n",
    "$$w_i^T u_k + b_i + b_k = log(X_{ik})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37f43a",
   "metadata": {},
   "source": [
    "**Defining a cost**\n",
    "\n",
    "In an ideal setting, where you have perfect word vectors, the above expression will be zero. In other words, that’s our goal or objective. So we will be setting the LHS expression as our cost function.\n",
    "\n",
    "$$J(w_i, w_j)= (w_i^T u_j + b_i +b_j — log(X_{ij}))^2$$\n",
    "\n",
    "Note that the square makes this a mean square cost function. But unfortunately our work does not stop here, we still have to patch up an important theoretical problem. Ponder what would happen if $X_{ik} = 0$. If you kick off a little experiment with the above cost function we'll have a problem because $log(0)$ is undefined. The easy fix would be to use $log(1+X_{ik})$ known as Laplacian smoothing. But the authors behind the GloVe paper propose a new way of doing this. That is to introduce a weighting function.\n",
    "\n",
    "$$J = f(X_{ij}) \\left( w_i^T u_j + b_i +b_j — log(X_{ij})\\right)^2$$\n",
    "\n",
    "where \n",
    "\n",
    "$$f(X_{ij}) = (x/x_{max})^\\alpha \\quad \\text{if} \\, x < x_{max} \\quad \\text{else} \\, 0$$\n",
    "\n",
    "(in the original paper you can find some considerations about the value of tha $\\alpha$ parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1707db2",
   "metadata": {},
   "source": [
    "## Pre-Trained Embedding Models\n",
    "\n",
    "In practice, we use both GloVe and Word2Vec to convert our text into embeddings and both exhibit comparable performances. \n",
    "Although in real applications we train our model over Wikipedia text with a window size around 5- 10. \n",
    "The number of words in the corpus is around 13 million, hence it takes a huge amount of time and resources to generate these embeddings. \n",
    "To avoid this we can use the pre-trained word vectors that are already trained and we can easily use them. \n",
    "Here are the links to download pre-trained Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d6848",
   "metadata": {},
   "source": [
    "# References and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e7ef5",
   "metadata": {},
   "source": [
    "***Bird S. et al.***, \"*Natural Language Processing with Python*\" O'Reilly (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c93f4c",
   "metadata": {},
   "source": [
    "***Bengfort B. et al.***, \"*Applied Text Analysis with Python*\" O'Reilly (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440b6f5",
   "metadata": {},
   "source": [
    "***Sarkar D.***, \"*Text Analytics with Python A Practitioner's Guide to Natural Language Processing 2nd Ed*\" APress (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f78b09",
   "metadata": {},
   "source": [
    "***Pennington J. et al.***, \"*GloVe: Global Vectors for Word Representation*\", [pdf](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11c946",
   "metadata": {},
   "source": [
    "***Mikolov T. et al.***, \"*Efficient Estimation of Word Representations in Vector Space*\", arXiv:1301.3781v3  7 Sep 2013 [pdf](https://arxiv.org/abs/1301.3781)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": "6",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
