{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a081bb3f",
   "metadata": {},
   "source": [
    "# Basic Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff6922",
   "metadata": {},
   "source": [
    "## What is Text Mining and Why it's so Important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8478f1",
   "metadata": {},
   "source": [
    "Text is one of the most widespread forms of sequence data. It can be understood as\n",
    "either a sequence of characters or a sequence of words, but it’s most common to work\n",
    "at the level of words. According to industry estimates, only 21% of the available data is present in a structured form. Data is being generated as we speak, as we tweet, as we send messages on WhatsApp and in various other activities. The majority of this data exists in the textual form, which is highly unstructured in nature. \n",
    "\n",
    "Despite having high dimension data, the information present in it is not directly accessible unless it is processed (read and understood) manually or analyzed by an automated system. In order to produce significant and actionable insights from text data, it is important to get acquainted with the basics of Text Analysis.\n",
    "\n",
    "Text Mining is the process of analysis of texts written in natural language and extract high-quality information from text. It involves looking for interesting patterns in the text or to extract data from the text to be inserted into a database. Text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities). Developers have to prepare text using lexical analysis, POS (Parts-of-speech) tagging, stemming and other Natural Language Processing techniques to gain useful information from text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24865262",
   "metadata": {},
   "source": [
    "## Package Required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dece3b",
   "metadata": {},
   "source": [
    "**To start**, install the packages you need to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791ed4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Regular Expression (RegEx)\n",
    "import re\n",
    "# Operating System Module\n",
    "import os\n",
    "# numpy library\n",
    "import numpy as np\n",
    "# pandas library\n",
    "import pandas as pd\n",
    "# matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "# if uising a Jupyter notebook, include:\n",
    "%matplotlib inline\n",
    "# Natural Language Toolkit \n",
    "import nltk\n",
    "# The BeautifulSoup Library for WEB Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import codecs\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19227b6",
   "metadata": {},
   "source": [
    "## What is a Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af30165",
   "metadata": {},
   "source": [
    "A Corpus is defined as a **collection of text documents** for example a data set containing news is a corpus or the tweets containing Twitter data is a corpus. So corpus consists of documents, documents comprise paragraphs, paragraphs comprise sentences and sentences comprise further smaller units which are called Tokens.\n",
    "\n",
    "Acquiring a domain-specific corpus will be essential to producing a languare-aware data product that performs well. Naturally the next question should then be \"How do we construct a dataset with wchich to build a language model?\". \n",
    "\n",
    "While in the next chapters we will see how to use existing corpus, it is still necessary to give a brief hint on how to extract texts from the web independently and which are the main libraries that we can use for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43280c",
   "metadata": {},
   "source": [
    "## Processing Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda06b7",
   "metadata": {},
   "source": [
    "The most important source of texts is undoubtedly the Web. It's convenient to have existing text collections to explore, such as the corpora we 'll see in the next paragraph. However, you probably have your own text sources in mind, and need to learn how to access them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af40fa9",
   "metadata": {
    "id": "ZCSLsffX1HLv"
   },
   "source": [
    "### Processing HTML Files\n",
    "\n",
    "The first type of structured text document you’ll look at is HTML—a markup\n",
    "language commonly used on the web for human-readable representation of\n",
    "information. An HTML document consists of text and predefined tags (enclosed\n",
    "in angle brackets <>) that control the presentation and interpretation of the\n",
    "text. The tags may have attributes.\n",
    "\n",
    "Reference: [this](https://towardsdatascience.com/choose-the-best-python-web-scraping-library-for-your-application-91a68bc81c4f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731085c",
   "metadata": {},
   "source": [
    "### Urllib\n",
    "\n",
    "Urllib is a Python library that allows the developer to open and parse information from HTTP or FTP protocols. Urllib offers some functionality to deal with and open URLs, namely:\n",
    "- urllib.request: opens and reads URLs.\n",
    "- urllib.error: catches the exceptions raised by urllib.request.\n",
    "- urllib.parse: parses URLs.\n",
    "- urllib.robotparser: parses robots.txt files.\n",
    "\n",
    "You don’t need to install Urllib since it is a part of the built-in Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fcf5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "def freq_words_2(url, n):\n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    text = BeautifulSoup(html, 'html.parser').get_text()\n",
    "    fd = nltk.FreqDist(word.lower() for word in nltk.word_tokenize(text))\n",
    "    return [word for (word, _) in fd.most_common(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b52a4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', 'and', '.', 'to']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = \"https://static.nytimes.com/email-content/CB_sample.html\"\n",
    "freq_words_2(page, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c72d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "def freq_words_3(url, n):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    html = request.urlopen(url).read().decode('utf8')\n",
    "    text = BeautifulSoup(html, 'html.parser').get_text()\n",
    "    fd = nltk.FreqDist(word.lower() for word in nltk.word_tokenize(text) if not word in stop_words and word.isalpha())\n",
    "    return [word for (word, _) in fd.most_common(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37836eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new', 'york', 'i', 'the', 'time']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words_3(page, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97948223",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8629801",
   "metadata": {
    "id": "ZpJ5ThgTvBEC"
   },
   "source": [
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a Python library that is used to extract information from XML and HTML files. Beautiful Soup is considered a parser library. Parsers help the programmer obtain data from an HTML file. One of Beautiful Soup’s strengths is its ability to detect page encoding, and hence get more accurate information from the HTML text. Another advantage of Beautiful Soup is its simplicity and ease.\n",
    "\n",
    "You can construct a BeautifulSoup object from a markup\n",
    "string, a markup file, or a URL of a markup document on the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e03acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "«headers»«body»\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Construct soup from a string\n",
    "soup1 = BeautifulSoup(\"<HTML><HEAD>«headers»</HEAD>«body»</HTML>\")\n",
    "print(soup1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38211295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Democrats Poised For Senate Control As Counting Continues In Georgia : NPR</title>\n"
     ]
    }
   ],
   "source": [
    "# Construct soup from a local file\n",
    "soup2 = BeautifulSoup(open(\"./corpus/web/Democrats Poised For Senate Control As Counting Continues In Georgia_NPR.html\"))\n",
    "print(soup2.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc709e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrats Poised For Senate Control As Counting Continues In Georgia : NPR\n"
     ]
    }
   ],
   "source": [
    "print(soup2.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113e84f",
   "metadata": {},
   "source": [
    "One common task is extracting all the URLs found within a page’s <a> tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfb4e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup2.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af7000",
   "metadata": {},
   "source": [
    "**When to use BeautifulSoup?**\n",
    "If you’re just starting with webs scarping or with Python, Beautiful Soup is the best choice to go. Moreover, if the documents you’ll be scraping are not structured, Beautiful Soup will be the perfect choice to use.\n",
    "If you’re building a big project, Beautiful Soup will not be the wise option to take. Beautiful Soup projects are not flexible and are difficult to maintain as the project size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c75fb08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Americans Stretch Across Political Divides to Welcome Afghan Refugees\n",
      "Taliban Claim Control Over Panjshir Valley; Resistance Vows to Fight On\n",
      "A military analysis raised questions about a deadly drone strike on a car in Kabul last Sunday.\n",
      "Oregon and Idaho Are Running Out of I.C.U. Beds as Covid Cases Hit Records\n",
      "Here’s why you might not be returning to the office until next year.\n",
      "Congregations are marking the Jewish High Holy Days in the shadow of Covid, again.\n",
      "Tracking the Coronavirus ›\n",
      "As Migrants Surge Toward Border, Court Hands Biden a Lifeline\n",
      "An Unsung Pit Crew of California’s Wildfires: Hotel Workers Left Behind\n",
      "See the Caldor fire’s march to the edge of South Lake Tahoe.\n",
      "They Put Everything Into Their Homes. Not One Was Spared in the Flood.\n",
      "California Bill Could Alter Amazon Labor Practices\n",
      "The bill would rein in production quotas at warehouses that critics say are excessive and force workers to forgo bathroom breaks.\n",
      "The Strange Tale of the Freedom Phone, a Smartphone for Conservatives\n",
      "A 22-year-old Bitcoin millionaire wants Republicans to ditch their iPhones for a low-end handset that he hopes to turn into a political tool.\n",
      "Can Anyone in the N.F.C. Stop Tom Brady and the Bucs From Repeating?\n",
      "Rivalries and Rookies Bloom, but It’s Still Patrick Mahomes’s A.F.C.\n",
      "Gail Collins and Bret StephensHistory Can Close in on Us Awfully Fast\n",
      "\n",
      "Margaret RenklSouthern Republicans Cannot Be Trusted With Public Health\n",
      "Terri GersteinOther People’s Rotten Jobs Are Bad for Them. And for You.\n",
      "Charles M. BlowFrom ‘Ku Kluxism’ to Trumpism\n",
      "\n",
      "Maya GuzdarWhat Happened the Day After I Was Sexually Harassed at the Pentagon\n",
      "Virginia López GlassVenezuela’s New Lettuce-Based Economy Is Good Enough for Now\n",
      "Kelly CorriganHow to Let Go of Your Irreplaceable, Unstoppable Daughter\n",
      "The New York TimesJoin Kara Swisher for a Live Event\n",
      "Coach Coughlin and the Caregiver’s Love and Heartbreak\n",
      "Jerome KarabelLet’s Honor the True Spirit of Labor Day\n",
      "Maureen DowdDrowning Our Future in the Past\n",
      "Jamelle BouieHow Has Joe Biden Become So Unpopular?\n",
      "Chris Windsor\n",
      "\n",
      "\n",
      "Listen to ‘Popcast’\n",
      "Sign Up for the Watching Newsletter\n",
      "Opinion\n",
      "Virtual Event: Kids and Covid\n",
      "Ex-Marine Sharpshooter Kills 4 and Fires at Deputies in Florida, Sheriff Says\n",
      "If Gawker Is Nice, Is It Still Gawker?\n",
      "Endangered Birds Have a Defender on N.Y.C. Beaches: The Plover Patrol\n",
      "Texas Abortion Law Is the Culmination of Decades of Conservative Effort\n",
      "Belarus Opposition Leader Sentenced to 11 Years in Prison\n",
      "Chinese Social Media Site Suspends K-Pop Fan Accounts\n",
      "Kanye West’s ‘Donda’ Is No. 1, but Drake Waits in the Wings\n",
      "Jacob Zuma, South Africa’s Ex-President, Gets Medical Parole\n",
      "Hurricane Larry Is Forecast to Bring Rough Surf to East Coast\n",
      "Is Your ‘Go Bag’ Ready?\n",
      "What Vaccinated People Need to Know About Breakthrough Infections\n",
      "The Secret to Raising a Resilient Kid\n",
      "How Burnout Became the Norm for American Parents\n",
      "5-Minute Stress Resets to Try\n",
      "Tony Cenicola/The New York Times\n",
      "John G Mabanglo/EPA, via Shutterstock\n",
      "Charlotte Hadden for The New York Times\n",
      "Jeenah Moon for The New York Times\n",
      "Marcio Jose Sanchez/Associated Press\n",
      "Our Best Rosh Hashana and Yom Kippur Recipes\n",
      "Bombay Frittata\n",
      "Sheet-Pan Salmon and Eggplant With XO Sauce\n",
      "Apple Jelly\n",
      "Zucchini Panzanella\n",
      "The Best Kids Headphones\n",
      "Keep Your Tent Out of the Sun\n",
      "Everything You Need to Make Hot Pot at Home\n",
      "Learn More About Wirecutter\n",
      "Spelling Bee\n",
      "The Crossword\n",
      "Letter Boxed\n",
      "Tiles\n",
      "Vertex\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "base_url = 'http://www.nytimes.com'\n",
    "r = requests.get(base_url)\n",
    "soup = BeautifulSoup(r.text)\n",
    " \n",
    "for story_heading in soup.find_all(class_=\"story-wrapper\"): \n",
    "    #print(story_heading)\n",
    "    if story_heading.a: \n",
    "        print(story_heading.a.text.replace(\"\\n\", \" \").strip())\n",
    "    else: \n",
    "        print(story_heading.contents[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b1b6858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taliban fighters pictured outside governor’s compound, but Ahmad Massoud’s rebels deny province has fallen\n",
      "Former world leaders and public figures say nationwide marches are modelled on US Capitol insurrection\n",
      "Real Vishnevsky battling two doppelgängers who seem to have changed their appearance as well as their names\n",
      "Storm wreaked havoc on offshore oil production platforms and onshore oil and gas processing plants\n",
      "Recent discoveries of mass graves have shed new light on the country’s troubled colonial legacy\n",
      "Immortalised by Godard and Melville, the actor specialised in seductive tough guys – and blazed a trail through cinema history\n",
      "After building his own version of Middle-earth, Nicolas Gentile has thrown a ‘ring’ into Mount Vesuvius\n",
      "Mississippi is least vaccinated state in US; New Zealand reports 20 new cases for third day\n",
      "Some welcome the routine, while others are concerned about how it will work without everyone in at the same time\n",
      "England face a record run chase to beat India in the fourth Test. Join our writers for updates\n",
      "Giovani Lo Celso and Cristian Romero are in hot water with Tottenham after going against the club’s wishes to play in the abandoned World Cup qualifier between Brazil and Argentina\n",
      "Tony Estanguet has already set up community programmes and wants to ‘transform the city of Paris into a big Olympic and Paralympic park’\n",
      "From plummeting trade to drastic shortages of workers, needlessly leaving the single market has been disastrous, says Guardian columnist Simon Jenkins\n",
      "Those responsible for 20 years of strategic disaster won’t actually be held responsible, says former military officer Frank Ledwidge\n",
      "It may be the first Marvel film not to be approved in mainland China, but its American Chinese heritage hero and strong female representation are badass\n",
      "Developers are harming cities like London and New York by asking the public to visit and help monetise their monstrosities\n",
      "A warning call told residents of al-Jalaa apartment block that their homes were about to be destroyed. This is the story of the frantic evacuation that followed – told through recordings made by the people who lived there\n",
      "With a tight campaign devised by a sports marketer and his face all over the streets, the SPD leader looks set to win\n",
      "House in southern France yielded find of outstanding wall paintings dating from 1st century BC\n",
      "20 men accused of involvement in 2015 massacre, but it is unclear whether the key accused will break their silence\n",
      "An innovative community project has brightened buildings, ‘brought people together’ and provided an emotional outlet after traumatic journeys\n",
      "Editorial in publications worldwide urges leaders to take measures to stop ‘greatest threat to public health’\n",
      "The figure for the three months to September was 1.3C above the long-term average and higher than the previous record set in 2020\n",
      "World’s largest lizard moves from vulnerable to endangered on IUCN red list of threatened species\n",
      "ShareAction says lack of plans to tackle climate crisis and biodiversity loss casts doubts on banking’s sustainability pledges\n",
      "Angela Merkel ‘profoundly shaken’ by death of Jan Hecker, 54, one of her top foreign policy advisers\n",
      "English-born entertainer, 74, has cancelled her appearance at American film festival in France\n",
      "Researchers solve mystery of why southern hemisphere whales switch suddenly but in north it is gradual\n",
      "As Elon Musk launches his general-purpose humanoid, a look at some of movie’s most helpful robots, from Alita to R2-D2\n",
      "Netflix’s new documentary series sells the first all-civilian flight to space as an exercise in philanthropy, but it’s little more than a privilege-fuelled puff piece for the billionaire’s adventures\n",
      "Todd, 47, and India, 27, met when she walked into his office on her police duties. They now live together in St Augustine, Florida\n",
      "A delicious fusion of Spanish and Italian cultures meld in this tomatoey noodle dish infused with chipotle and topped with soured cream and salsa\n",
      "Find out why this beautifully rustic bean and pasta offcut stew is the ‘Bill Murray of foods’\n",
      "A week after the storm hit, thousands of residents face a continuing catastrophe amid faltering federal government assistance\n",
      "During lockdown, more than 60% of us started new creative pursuits. Here’s how to keep them up as autumn gets under way\n",
      "Colleges offer support as young people aim to devote their lives to battling the crisis\n",
      "We’d like to hear from those living in Australia who have tested positive recently for Covid or are recovering\n",
      "From the joyful to the sad, we would like to hear your stories about a point in time when your world turned on its axis\n",
      "The long-running series in which readers answer other readers’ questions on subjects ranging from trivial flights of fancy to profound scientific and philosophical concepts\n",
      "You can send a news tip direct to Guardian journalists here. For stories that need a high level of security then contact us here\n",
      "The Guardian’s picture editors select photo highlights from around the world\n",
      "The Someone’s Daughter exhibition, portraits of women involved with the criminal justice system, launches at Photo London, special exhibitors section, on 9 September\n",
      "The singer, model, actor and former member of the group Girls Aloud has died aged 39 after being diagnosed with breast cancer\n",
      "Tom Wood captures a moment of pilgrimage overlooking his birthplace of County Mayo\n",
      "Michael Rosen, Paloma Faith, St Etienne and ‘champing’ holidays – the best photography commissioned by the Observer in August 2021\n",
      "Artist Melanie Issaka’s photograms explore what it means to be black, British and female\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.theguardian.com/international'\n",
    "r    = requests.get(base_url)\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "for story_heading in soup.find_all(class_=\"fc-item__standfirst\"): \n",
    "    #print(story_heading)\n",
    "    if story_heading.a: \n",
    "        print(story_heading.a.text.replace(\"\\n\", \" \").strip())\n",
    "    else: \n",
    "        print(story_heading.contents[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f27cb",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fecff2",
   "metadata": {},
   "source": [
    "A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern. RegEx can be used to check if a string contains the specified search pattern. Python has a built-in package called re, which can be used to work with Regular Expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c513f09",
   "metadata": {},
   "source": [
    "### Why we need Regular Expression\n",
    "\n",
    "Imagine you have a string object s. Now suppose you need to write Python code to find out whether s contains the substring '123'. There are at least a couple ways to do this. You could use the in operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4eb6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'foo123bar'\n",
    "'123' in s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4c35e",
   "metadata": {},
   "source": [
    "If you want to know not only whether '123' exists in s but also where it exists, then you can use .find() or .index(). Each of these returns the character position within s where the substring resides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c47ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.find('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d419ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.index('123')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0afc7",
   "metadata": {},
   "source": [
    "In these examples, the matching is done by a straightforward character-by-character comparison. That will get the job done in many cases. But sometimes, the problem is more complicated than that.\n",
    "\n",
    "For example, rather than searching for a fixed substring like '123', suppose you wanted to determine whether a string contains any three consecutive decimal digit characters, as in the strings 'foo123bar', 'foo456bar', '234baz', and 'qux678'.\n",
    "\n",
    "Strict character comparisons won’t cut it here. This is where regexes in Python come to the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638138c9",
   "metadata": {},
   "source": [
    "### The Re Module\n",
    "\n",
    "Regex functionality in Python resides in a module named re. \n",
    "\n",
    "For now, you’ll focus predominantly on one function, `re.search()`.\n",
    "\n",
    "`re.search(\\<regex>, \\<string>)`\n",
    "\n",
    "This function search looking for the first location where the pattern \\<regex> matches. If a match is found, then `re.search()` returns a match object. Otherwise, it returns `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3a8c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 6), match='123'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.search('123', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b56f0",
   "metadata": {},
   "source": [
    "For the moment, the important point is that re.search() did in fact return a match object rather than None. That tells you that it found a match. In other words, the specified \\<regex> pattern 123 is present in s. The interpreter displays the match object as \\<_sre.SRE_Match object; span=(3, 6), match='123'>. \n",
    "\n",
    "This contains some useful information:\n",
    "\n",
    "- span=(3, 6) indicates the portion of <string> in which the match was found. In this example, the match starts at character position 3 and extends up to but not including position 6.\n",
    "    \n",
    "- match='123' indicates which characters from \\<string> matched.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29bb4f",
   "metadata": {},
   "source": [
    "### Python Regex Metacharacters\n",
    "\n",
    "The real power of regex matching in Python emerges when \\<regex> contains special characters called metacharacters. These have a unique meaning to the regex matching engine and vastly enhance the capability of the search. Consider again the problem of how to determine whether a string contains any three consecutive decimal digit characters.\n",
    "\n",
    "In a regex, a set of characters specified in square brackets ([]) makes up a character class. This metacharacter sequence matches any single character that is in the class, as demonstrated in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c38d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdash5622hjj\n",
      "654fdhaskjf\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# [0-9] matches any single decimal digit character—any character between '0' and '9', inclusive. \n",
    "# The full expression [0-9][0-9][0-9] matches any sequence of three decimal digit characters.\n",
    "# On the other hand, a string that doesn’t contain three consecutive digits won’t match!\n",
    "#\n",
    "pattern = '[0-9][0-9][0-9]'\n",
    "\n",
    "mylist = ['gdash5622hjj', 'dafasfas', '654fdhaskjf', 'ashjdfuqo','67yahd', '9jhdksaf', '42hddhdh67','udyakh']\n",
    "for l in mylist:\n",
    "    if re.search(pattern, l):\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52137f2",
   "metadata": {},
   "source": [
    "With regexes in Python, you can identify patterns in a string that you wouldn’t be able to find with the in operator or with string methods.\n",
    "\n",
    "Take a look at another regex metacharacter. The dot (.) metacharacter matches any character except a newline, so it functions like a wildcard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8549f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdash5622hjj\n",
      "ashjdfuqo\n",
      "udyakh\n"
     ]
    }
   ],
   "source": [
    "pattern = 'a.h'\n",
    "for l in mylist:\n",
    "    if re.search(pattern, l):\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c403e6",
   "metadata": {},
   "source": [
    "Here, you’re essentially asking, *“Does s contain a 'a', then any character (except a newline), then a 'h'?”*.\n",
    "\n",
    "A character class metacharacter sequence will match any single character contained in the class. You can enumerate the characters individually like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b534aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdas askjd Nr59 dsafh\n",
      "Nr47 adfd jads\n"
     ]
    }
   ],
   "source": [
    "pattern = 'Nr[0-9]'\n",
    "\n",
    "mylist = ['gdas askjd Nr59 dsafh', 'dafasfas', 'Nr47 adfd jads', 'dkajòqwo idf Nr 78','67yahd', '9jhdksaf', '42hddhdh67','udyakh']\n",
    "for l in mylist:\n",
    "    if re.search(pattern, l):\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0a0a6",
   "metadata": {},
   "source": [
    "You can complement a character class by specifying ^ as the first character, in which case it matches any character that isn’t in the set. In the following example, [^0-9] matches any character that isn’t a digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2139bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All in all, the EU economy is forecast to grow by 4.6% in and to strengthen to around 5.3% in 4.2% and 3.2% respectively, in the euro area. "
     ]
    }
   ],
   "source": [
    "pattern = '[^0-9].'\n",
    "\n",
    "words = 'All in all, the EU economy is forecast to grow by 4.6% in 2021 and \\\n",
    "         to strengthen to around 5.3% in 2022, 4.2% and 3.2% respectively, in the euro area.'\n",
    "\n",
    "#words = words.replace('.','').replace(',','')\n",
    "\n",
    "mylist = words.split()\n",
    "for l in mylist:\n",
    "    if re.search(pattern, l):\n",
    "        print(l, end= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b88b6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'l', 'l', 'i', 'n', 'a', 'l', 'l', ',', 't', 'h', 'e', 'E', 'U', 'e', 'c', 'o', 'n', 'o', 'm', 'y', 'i', 's', 'f', 'o', 'r', 'e', 'c', 'a', 's', 't', 't', 'o', 'g', 'r', 'o', 'w', 'b', 'y', '4', '.', '6', '%', 'i', 'n', '2', '0', '2', '1', 'a', 'n', 'd', 't', 'o', 's', 't', 'r', 'e', 'n', 'g', 't', 'h', 'e', 'n', 't', 'o', 'a', 'r', 'o', 'u', 'n', 'd', '5', '.', '3', '%', 'i', 'n', '2', '0', '2', '2', ',', '4', '.', '2', '%', 'a', 'n', 'd', '3', '.', '2', '%', 'r', 'e', 's', 'p', 'e', 'c', 't', 'i', 'v', 'e', 'l', 'y', ',', 'i', 'n', 't', 'h', 'e', 'e', 'u', 'r', 'o', 'a', 'r', 'e', 'a', '.']\n",
      "Allinall,theEUeconomyisforecasttogrowby4.6%in2021andtostrengthentoaround5.3%in2022,4.2%and3.2%respectively,intheeuroarea.\n"
     ]
    }
   ],
   "source": [
    "# match all non-blanck characters\n",
    "y = re.findall('[^ ]',words)\n",
    "print(y)\n",
    "# a simple way to remove all blanck space in a string\n",
    "print(''.join(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06713458",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "804a03e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zuccarino',\n",
       " 'zucchetto',\n",
       " 'zucchini',\n",
       " 'zudda',\n",
       " 'zugtierlast',\n",
       " 'zugtierlaster',\n",
       " 'zuisin',\n",
       " 'zumatic',\n",
       " 'zumbooruk',\n",
       " 'zunyite',\n",
       " 'zupanate',\n",
       " 'zuurveldt',\n",
       " 'zuza']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^zu', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d3383",
   "metadata": {},
   "source": [
    "Let's find words ending with **zz** using the regular expressio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fd16ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abuzz',\n",
       " 'bejazz',\n",
       " 'bizz',\n",
       " 'blizz',\n",
       " 'brizz',\n",
       " 'bruzz',\n",
       " 'buzz',\n",
       " 'fizz',\n",
       " 'frizz',\n",
       " 'fuzz',\n",
       " 'gizz',\n",
       " 'hizz',\n",
       " 'humbuzz',\n",
       " 'huzz',\n",
       " 'jazz',\n",
       " 'muzz',\n",
       " 'outbuzz',\n",
       " 'outjazz',\n",
       " 'razz',\n",
       " 'sizz',\n",
       " 'unfrizz',\n",
       " 'zizz']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('zz$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f556568c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appearanced',\n",
       " 'appellatived',\n",
       " 'appendaged',\n",
       " 'appendiculated',\n",
       " 'applied',\n",
       " 'appressed']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^app.*ed$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "008f3cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aaaaaaaaaaaaaaaaa',\n",
       " 'aaahhhh',\n",
       " 'ah',\n",
       " 'ahah',\n",
       " 'ahahah',\n",
       " 'ahh',\n",
       " 'ahhahahaha',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhhh',\n",
       " 'ahhhhhhhhhhhhhh',\n",
       " 'h',\n",
       " 'ha',\n",
       " 'haaa',\n",
       " 'hah',\n",
       " 'haha',\n",
       " 'hahaaa',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hahahaa',\n",
       " 'hahahah',\n",
       " 'hahahaha',\n",
       " 'hahahahaaa',\n",
       " 'hahahahahaha',\n",
       " 'hahahahahahaha',\n",
       " 'hahahahahahahahahahahahahahahaha',\n",
       " 'hahahhahah',\n",
       " 'hahhahahaha']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "[w for w in chat_words if re.search('^[ha]+$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8afe06",
   "metadata": {},
   "source": [
    "It should be clear that + simply means \"one or more instances of the preceding item\", which could be an individual character like m, a set like [fed] or a range like [d-f]. Now let's replace + with *, which means \"zero or more instances of the preceding item\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d1dab4",
   "metadata": {},
   "source": [
    "### Further Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1c1f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=['X-Sieve: CMU Sieve 2.3', 'X-DSPAM-Result: Innocent', 'X-Plane is behind schedule: two weeks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90df62dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X-Sieve:']\n",
      "['X-DSPAM-Result:']\n",
      "['X-Plane is behind schedule:']\n"
     ]
    }
   ],
   "source": [
    "regex = r'^X.*:'\n",
    "for t in text:\n",
    "    y = re.findall(regex, t)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b18948d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X-Sieve:']\n",
      "['X-DSPAM-Result:']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "regex = r'^X-\\S+:'\n",
    "for t in text:\n",
    "    y = re.findall(regex, t)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71112e11",
   "metadata": {},
   "source": [
    "Greedy and Lazy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b1517b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: Using the :']\n"
     ]
    }
   ],
   "source": [
    "# greedy\n",
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+:', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded7015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From:']\n"
     ]
    }
   ],
   "source": [
    "# lazy (note the question mark before ':')\n",
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+?:', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a7960a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za,', 'giovanni.dellalunga@unibo.it']\n"
     ]
    }
   ],
   "source": [
    "text = \"From stephen.marquard@uct.ac.za, giovanni.dellalunga@unibo.it Sat Jan  5 09:14:16 2008\"\n",
    "\n",
    "y = re.findall(r'\\S+@\\S+',text)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d013923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za,']\n"
     ]
    }
   ],
   "source": [
    "y = re.findall(r'^From (\\S+@\\S+)', text)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "739cbaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unibo.it']\n"
     ]
    }
   ],
   "source": [
    "text  = \"From giovanni.dellalunga@unibo.it Sat Jan  5 09:14:16 2008\"\n",
    "regex = r'^From .*@([^ ]*)'\n",
    "\n",
    "y = re.findall(regex, text)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ad0a7",
   "metadata": {},
   "source": [
    "## References and Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c4a90",
   "metadata": {},
   "source": [
    "***Bird S. et al.***, \"*Natural Language Processing with Python*\" O'Reilly (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec1769",
   "metadata": {},
   "source": [
    "***Bengfort B. et al.***, \"*Applied Text Analysis with Python*\" O'Reilly (2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
